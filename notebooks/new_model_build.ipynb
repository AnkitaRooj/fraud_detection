{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31343c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import IsolationForest, GradientBoostingClassifier\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                           precision_score, recall_score, f1_score, accuracy_score)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a70c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processeddataset/final_feature_paySim.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b2701",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud = df[df['isFraud']==1]\n",
    "non_fraud = df[df['isFraud']==0].sample(n=fraud.shape[0], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([fraud, non_fraud]).reset_index(drop=True)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest',\n",
    "'hour', 'is_weekend', 'high_risk_hour', 'high_risk_type', 'large_amount_flag',\n",
    "'zero_balance_orig', 'zero_balance_dest', 'balance_ratio_orig', 'balance_ratio_dest',\n",
    "'cust_avg_amt', 'cust_std_amt', 'cust_txn_count'\n",
    "]\n",
    "X = df[feature_columns]\n",
    "y = df['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original class distribution:\", Counter(y))\n",
    "print(f\"Fraud rate: {y.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5156403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa43934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_scores, model_name):\n",
    "\n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Business metrics\n",
    "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    true_positive_rate = recall  # Same as recall\n",
    "    \n",
    "    # Classification report\n",
    "    cr = classification_report(y_true, y_pred, output_dict=True)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'confusion_matrix': cm,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'false_negative_rate': false_negative_rate,\n",
    "        'true_positive_rate': true_positive_rate,\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'true_negatives': tn,\n",
    "        'false_negatives': fn,\n",
    "        'classification_report': cr,\n",
    "        'predictions': y_pred,\n",
    "        'scores': y_scores\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
    "    print(f\"False Negative Rate: {false_negative_rate:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f4fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_to_mlflow(model, metrics, model_name, params):\n",
    "\n",
    "    # Log parameters\n",
    "    for param, value in params.items():\n",
    "        mlflow.log_param(param, value)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", metrics['accuracy'])\n",
    "    mlflow.log_metric(\"precision\", metrics['precision'])\n",
    "    mlflow.log_metric(\"recall\", metrics['recall'])\n",
    "    mlflow.log_metric(\"f1_score\", metrics['f1_score'])\n",
    "    mlflow.log_metric(\"roc_auc\", metrics['roc_auc'])\n",
    "    mlflow.log_metric(\"false_positive_rate\", metrics['false_positive_rate'])\n",
    "    mlflow.log_metric(\"false_negative_rate\", metrics['false_negative_rate'])\n",
    "    mlflow.log_metric(\"true_positive_rate\", metrics['true_positive_rate'])\n",
    "    mlflow.log_metric(\"true_positives\", metrics['true_positives'])\n",
    "    mlflow.log_metric(\"false_positives\", metrics['false_positives'])\n",
    "    mlflow.log_metric(\"true_negatives\", metrics['true_negatives'])\n",
    "    mlflow.log_metric(\"false_negatives\", metrics['false_negatives'])\n",
    "    \n",
    "    # Log classification report as JSON\n",
    "    mlflow.log_dict(metrics['classification_report'], \"classification_report.json\")\n",
    "    \n",
    "    # Log confusion matrix as plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(plt.gcf(), f\"confusion_matrix_{model_name.replace(' ', '_')}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, name = f\"model_{model_name.replace(' ', '_')}\",\n",
    "                             input_example=X_train.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a5a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "mlflow.set_experiment(\"Isolation_Forest_Hyperparameter_Tuning_USing_RandomizedSearchCV\")\n",
    "def randomized_search_tuning(X_train, X_test, y_test, n_iter=5):\n",
    "    # Define parameter distribution\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(50, 300),\n",
    "        'contamination': uniform(0.001, 0.1),  # 0.1% to 10%\n",
    "        'max_samples': uniform(0.5, 0.5),  # 0.5 to 1.0\n",
    "        'max_features': uniform(0.5, 0.5)   # 0.5 to 1.0\n",
    "    }\n",
    "    \n",
    "    # Create Isolation Forest model\n",
    "    iso_forest = IsolationForest(random_state=42, verbose=0)\n",
    "    \n",
    "    # Custom scorer for anomaly detection\n",
    "    from sklearn.metrics import make_scorer, f1_score\n",
    "    scorer = make_scorer(f1_score)\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"RandomizedSearch_IF\"):\n",
    "        # Perform randomized search\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=iso_forest,\n",
    "            param_distributions=param_dist,\n",
    "            n_iter=n_iter,\n",
    "            scoring=scorer,\n",
    "            cv=3,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        random_search.fit(X_train)\n",
    "        \n",
    "        # Log best parameters\n",
    "        mlflow.log_params(random_search.best_params_)\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = random_search.best_estimator_\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred = best_model.predict(X_test) # type: ignore\n",
    "        y_scores = best_model.decision_function(X_test) # type: ignore\n",
    "        y_pred_binary = (y_pred == -1).astype(int)\n",
    "        \n",
    "        # Calculate and log metrics\n",
    "        test_results = calculate_metrics(y_test, y_pred_binary, y_scores, \"Best Isolation Forest\")\n",
    "        for metric_name, metric_value in test_results.items():\n",
    "            if isinstance(metric_value, (int, float)):\n",
    "                mlflow.log_metric(f\"test_{metric_name}\", metric_value)\n",
    "        \n",
    "        # Log the best model\n",
    "        mlflow.sklearn.log_model(best_model, \"best_model\") # type: ignore\n",
    "        \n",
    "        # Log search results\n",
    "        mlflow.log_metric(\"best_cv_score\", random_search.best_score_)\n",
    "        \n",
    "        return best_model, random_search.best_params_, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_params_, test_results = randomized_search_tuning(X_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vfraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
